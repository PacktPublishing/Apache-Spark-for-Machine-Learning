{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b202174d-7deb-43b6-984c-efcecdc27de7",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa655c-749c-454c-89e3-43f6477089b0",
   "metadata": {},
   "source": [
    "### import the required libraries and create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b92f0b28-2fae-4497-9014-7541bde4baa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import the library and create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ClassificationExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5fb19-1907-4805-9e2c-58ec49e31081",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data from S3 and create dataframe\n",
    "### Display the first 3 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6760181f-1a4d-42d1-9860-6d7298ad2306",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+-----------------+---------------------+--------------------+---------+-------------+---------------+-----------------+------------------------+-------------+------------------+---------------+\n",
      "|UDI|Product ID|type|air_temperature_k|process_temperature_k|rotational_speed_rpm|torque_nm|tool_wear_min|machine_failure|tool_wear_failure|heat_dissipation_failure|power_failure|overstrain_failure|random_failures|\n",
      "+---+----------+----+-----------------+---------------------+--------------------+---------+-------------+---------------+-----------------+------------------------+-------------+------------------+---------------+\n",
      "|1  |M14860    |M   |298.1            |308.6                |1551                |42.8     |0            |0              |0                |0                       |0            |0                 |0              |\n",
      "|2  |L47181    |L   |298.2            |308.7                |1408                |46.3     |3            |0              |0                |0                       |0            |0                 |0              |\n",
      "|3  |L47182    |L   |298.1            |308.5                |1498                |49.4     |5            |0              |0                |0                       |0            |0                 |0              |\n",
      "+---+----------+----+-----------------+---------------------+--------------------+---------+-------------+---------------+-----------------+------------------------+-------------+------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.csv(\"s3a://test234/machine_failure_data.csv\", header=True, inferSchema=True)\n",
    "train_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba5b84e-16b3-4c1a-957c-078ba470dbc6",
   "metadata": {},
   "source": [
    "### Preprocessing the data: Convert all integer columns to float, remove null values and print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "34c5ab65-7e9a-472e-a48b-0cc3ddbed63d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UDI: float (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- air_temperature_k: double (nullable = true)\n",
      " |-- process_temperature_k: double (nullable = true)\n",
      " |-- rotational_speed_rpm: float (nullable = true)\n",
      " |-- torque_nm: double (nullable = true)\n",
      " |-- tool_wear_min: float (nullable = true)\n",
      " |-- machine_failure: float (nullable = true)\n",
      " |-- tool_wear_failure: float (nullable = true)\n",
      " |-- heat_dissipation_failure: float (nullable = true)\n",
      " |-- power_failure: float (nullable = true)\n",
      " |-- overstrain_failure: float (nullable = true)\n",
      " |-- random_failures: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert all integer columns in train_df to float\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "for col in train_df.columns:\n",
    "    if train_df.schema[col].dataType == IntegerType():\n",
    "        train_df = train_df.withColumn(col, train_df[col].cast(FloatType()))\n",
    "# remove null values\n",
    "train_df = train_df.dropna()\n",
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4473fd-92fc-4c7b-b09a-732dbe5f9259",
   "metadata": {},
   "source": [
    "# Prepare the data by removing the id columns and failure columns retaining only the machine failure column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c12d549c-9980-42fe-b068-d2457cdc4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+\n",
      "|type|air_temperature_k|process_temperature_k|rotational_speed_rpm|torque_nm|tool_wear_min|machine_failure|\n",
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+\n",
      "|M   |298.1            |308.6                |1551.0              |42.8     |0.0          |0.0            |\n",
      "|L   |298.2            |308.7                |1408.0              |46.3     |3.0          |0.0            |\n",
      "|L   |298.1            |308.5                |1498.0              |49.4     |5.0          |0.0            |\n",
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drope the id columns such as UDI, Product ID,heat_dissipation_failure,power_failure,overstrain_failure,random_failures,tool_wear_failure\n",
    "train_df = train_df.drop(\"UDI\",\"Product ID\",\"heat_dissipation_failure\",\"power_failure\",\"overstrain_failure\",\"random_failures\",\"tool_wear_failure\")\n",
    "train_df.show(3,truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48b539-26e8-4093-b421-4417c6df8918",
   "metadata": {},
   "source": [
    "# Feature Engineering: Fill missing values, convert the type column to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "47b9d8c5-3762-49a6-b58c-2c3fc9795910",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+----------+\n",
      "|type|air_temperature_k|process_temperature_k|rotational_speed_rpm|torque_nm|tool_wear_min|machine_failure|type_index|\n",
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+----------+\n",
      "|   M|            298.1|                308.6|              1551.0|     42.8|          0.0|            0.0|       1.0|\n",
      "|   L|            298.2|                308.7|              1408.0|     46.3|          3.0|            0.0|       0.0|\n",
      "|   L|            298.1|                308.5|              1498.0|     49.4|          5.0|            0.0|       0.0|\n",
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replace  the missing values with 0\n",
    "train_df = train_df.fillna(0)\n",
    "# convert the type column to index\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "train_df = indexer.fit(train_df).transform(train_df)\n",
    "train_df.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b529da1-98b9-4732-ae0e-d3dec4ef5faf",
   "metadata": {},
   "source": [
    "### Feature Engineering using vector assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a5f70749-8155-4bfc-8aa3-c4114310539f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            features|machine_failure|\n",
      "+--------------------+---------------+\n",
      "|[298.1,308.6,1551...|            0.0|\n",
      "|[298.2,308.7,1408...|            0.0|\n",
      "|[298.1,308.5,1498...|            0.0|\n",
      "+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"air_temperature_k\", \"process_temperature_k\", \"rotational_speed_rpm\", \"torque_nm\", \"tool_wear_min\",\"type_index\"], outputCol=\"features\")\n",
    "train_df = assembler.transform(train_df)\n",
    "train_df.select(\"features\", \"machine_failure\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26257b86-d07a-43e8-be21-5dd7383ca063",
   "metadata": {},
   "source": [
    "### Applying standard scaler and show the scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "988553d5-042a-40c2-afa1-76517a3ccc25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|      scaledFeatures|machine_failure|\n",
      "+--------------------+---------------+\n",
      "|[149.030724148837...|            0.0|\n",
      "|[149.080717682600...|            0.0|\n",
      "|[149.030724148837...|            0.0|\n",
      "+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply standard scaler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(train_df)\n",
    "train_df = scalerModel.transform(train_df)\n",
    "# show the scaled features\n",
    "train_df.select(\"scaledFeatures\", \"machine_failure\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8094b30-082a-4769-9b97-486c6fd010e6",
   "metadata": {},
   "source": [
    "### Split the data into train and test and select the features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e0e42a4a-ad75-430b-be91-7a9efd9b80b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data\n",
    "train_df, test_df = train_df.randomSplit([0.7, 0.3])\n",
    "# select the features and label\n",
    "train_df = train_df.select(\"scaledFeatures\", \"machine_failure\")\n",
    "test_df = test_df.select(\"scaledFeatures\", \"machine_failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23499bf6-f149-4b22-a237-c0f3bea457af",
   "metadata": {},
   "source": [
    "### Train the model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c5cadd8d-229a-43c6-8893-de9dee3a5a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"machine_failure\")\n",
    "lrModel = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914043a3-e0e8-4b49-9aca-65d87db22864",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9d2ad72e-327c-4d21-a213-83e4d1015ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918577395158542"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"machine_failure\")\n",
    "evaluator.evaluate(lrModel.transform(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9f071-b241-444c-873a-221f2b85f9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(hard-disk) Python",
   "language": "python",
   "name": "conda-env-hard-disk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
