{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9564867-d95a-426c-8411-eadd0190ff6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")  /FileStore/tables/train.csv\n",
    "Prem4753/Crime-classification-model-using-Pyspark/blob/main/crime_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a420f4-c144-4740-a8d6-3a6f3e69b56d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"crimes-classification\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28a3fbd5-3167-4e15-b3cb-0c04a34a6fb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "np.random.seed(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a65ac66-d260-4f50-af98-1b444ad1073e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Structure\n----------------------------------\nroot\n |-- Category: string (nullable = true)\n |-- Description: string (nullable = true)\n\nNone\n \nDataframe preview\n+--------------+--------------------+\n|      Category|         Description|\n+--------------+--------------------+\n|      warrants|      warrant arrest|\n|other offenses|traffic violation...|\n|other offenses|traffic violation...|\n| larceny/theft|grand theft from ...|\n| larceny/theft|grand theft from ...|\n+--------------+--------------------+\nonly showing top 5 rows\n\nNone\n \n----------------------------------\nTotal number of rows 878049\n"
     ]
    }
   ],
   "source": [
    "#Read the data into spark datafrome\n",
    "from pyspark.sql.functions import col, lower\n",
    "df = spark.read.format('csv')\\\n",
    "          .option('header','true')\\\n",
    "          .option('inferSchema', 'true')\\\n",
    "          .option('timestamp', 'true')\\\n",
    "          .load('/FileStore/tables/train.csv')\n",
    "\n",
    "data = df.select(lower(col('Category')),lower(col('Descript')))\\\n",
    "        .withColumnRenamed('lower(Category)','Category')\\\n",
    "        .withColumnRenamed('lower(Descript)', 'Description')\n",
    "data.cache()\n",
    "print('Dataframe Structure')\n",
    "print('----------------------------------')\n",
    "print(data.printSchema())\n",
    "print(' ')\n",
    "print('Dataframe preview')\n",
    "print(data.show(5))\n",
    "print(' ')\n",
    "print('----------------------------------')\n",
    "print('Total number of rows', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb05050-89f2-4b00-8722-f93c025b1440",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique value of Category: 39\n \nTop 10 Crime Category\n+--------------+----------+\n|      Category|totalValue|\n+--------------+----------+\n| larceny/theft|    174900|\n|other offenses|    126182|\n|  non-criminal|     92304|\n|       assault|     76876|\n| drug/narcotic|     53971|\n| vehicle theft|     53781|\n|     vandalism|     44725|\n|      warrants|     42214|\n|      burglary|     36755|\n|suspicious occ|     31414|\n+--------------+----------+\nonly showing top 10 rows\n\n \n \nTotal number of unique value of Description: 879\n \nTop 10 Crime Description\n+--------------------+----------+\n|         Description|totalValue|\n+--------------------+----------+\n|grand theft from ...|     60022|\n|       lost property|     31729|\n|             battery|     27441|\n|   stolen automobile|     26897|\n|drivers license, ...|     26839|\n|      warrant arrest|     23754|\n|suspicious occurr...|     21891|\n|aided case, menta...|     21497|\n|petty theft from ...|     19771|\n|malicious mischie...|     17789|\n+--------------------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "def top_n_list(df,var, N):\n",
    "    '''\n",
    "    This function determine the top N numbers of the list\n",
    "    '''\n",
    "    print(\"Total number of unique value of\"+' '+var+''+':'+' '+str(df.select(var).distinct().count()))\n",
    "    print(' ')\n",
    "    print('Top'+' '+str(N)+' '+'Crime'+' '+var)\n",
    "    df.groupBy(var).count().withColumnRenamed('count','totalValue')\\\n",
    "    .orderBy(col('totalValue').desc()).show(N)\n",
    "    \n",
    "    \n",
    "top_n_list(data, 'Category',10)\n",
    "print(' ')\n",
    "print(' ')\n",
    "top_n_list(data,'Description',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9694435-ff24-4386-9391-bfad448eef84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('Category').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5a2c9f-1d3b-4cd6-aa3c-83984fb0d526",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 614457\nTest Dataset Count: 263592\n"
     ]
    }
   ],
   "source": [
    "training, test = data.randomSplit([0.7,0.3], seed=60)\n",
    "#trainingSet.cache()\n",
    "print(\"Training Dataset Count:\", training.count())\n",
    "print(\"Test Dataset Count:\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a873f9fd-7857-4a32-a10c-5cbe3e6d744b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler, HashingTF, IDF, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes \n",
    "\n",
    "#----------------Define tokenizer with regextokenizer()------------------\n",
    "regex_tokenizer = RegexTokenizer(pattern='\\\\W')\\\n",
    "                  .setInputCol(\"Description\")\\\n",
    "                  .setOutputCol(\"tokens\")\n",
    "\n",
    "#----------------Define stopwords with stopwordsremover()---------------------\n",
    "extra_stopwords = ['http','amp','rt','t','c','the']\n",
    "stopwords_remover = StopWordsRemover()\\\n",
    "                    .setInputCol('tokens')\\\n",
    "                    .setOutputCol('filtered_words')\\\n",
    "                    .setStopWords(extra_stopwords)\n",
    "                    \n",
    "\n",
    "#----------Define bags of words using countVectorizer()---------------------------\n",
    "count_vectors = CountVectorizer(vocabSize=10000, minDF=5)\\\n",
    "               .setInputCol(\"filtered_words\")\\\n",
    "               .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "#-----------Using TF-IDF to vectorise features instead of countVectoriser-----------------\n",
    "hashingTf = HashingTF(numFeatures=10000)\\\n",
    "            .setInputCol(\"filtered_words\")\\\n",
    "            .setOutputCol(\"raw_features\")\n",
    "            \n",
    "#Use minDocFreq to remove sparse terms\n",
    "idf = IDF(minDocFreq=5)\\\n",
    "        .setInputCol(\"raw_features\")\\\n",
    "        .setOutputCol(\"features\")\n",
    "\n",
    "#---------------Define bag of words using Word2Vec---------------------------\n",
    "word2Vec = Word2Vec(vectorSize=1000, minCount=0)\\\n",
    "           .setInputCol(\"filtered_words\")\\\n",
    "           .setOutputCol(\"features\")\n",
    "\n",
    "#-----------Encode the Category variable into label using StringIndexer-----------\n",
    "label_string_idx = StringIndexer()\\\n",
    "                  .setInputCol(\"Category\")\\\n",
    "                  .setOutputCol(\"label\")\n",
    "\n",
    "#-----------Define classifier structure for logistic Regression--------------\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "#---------Define classifier structure for Naive Bayes----------\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "\n",
    "def metrics_ev(labels, metrics):\n",
    "    '''\n",
    "    List of all performance metrics\n",
    "    '''\n",
    "    # Confusion matrix\n",
    "    print(\"---------Confusion matrix-----------------\")\n",
    "    print(metrics.confusionMatrix)\n",
    "    print(' ')    \n",
    "    # Overall statistics\n",
    "    print('----------Overall statistics-----------')\n",
    "    print(\"Precision = %s\" %  metrics.precision())\n",
    "    print(\"Recall = %s\" %  metrics.recall())\n",
    "    print(\"F1 Score = %s\" % metrics.fMeasure())\n",
    "    print(' ')\n",
    "    # Statistics by class\n",
    "    print('----------Statistics by class----------')\n",
    "    for label in sorted(labels):\n",
    "       print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "       print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "       print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "    print(' ')\n",
    "    # Weighted stats\n",
    "    print('----------Weighted stats----------------')\n",
    "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0536601f-66e9-4358-b3a8-3b89cd98fd86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_cv_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, lr])\n",
    "model_cv_lr = pipeline_cv_lr.fit(training)\n",
    "predictions_cv_lr = model_cv_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f236c675-6860-49ba-806b-e69ebff6a14f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Check Top 5 predictions----------------------------------\n \n+------------------------------+-------------+------------------------------+-----+----------+\n|                   Description|     Category|                   probability|label|prediction|\n+------------------------------+-------------+------------------------------+-----+----------+\n|theft, bicycle, <$50, no se...|larceny/theft|[0.8717529184382807,0.02156...|  0.0|       0.0|\n|theft, bicycle, <$50, no se...|larceny/theft|[0.8717529184382807,0.02156...|  0.0|       0.0|\n|theft, bicycle, <$50, no se...|larceny/theft|[0.8717529184382807,0.02156...|  0.0|       0.0|\n|theft, bicycle, <$50, no se...|larceny/theft|[0.8717529184382807,0.02156...|  0.0|       0.0|\n|theft, bicycle, <$50, no se...|larceny/theft|[0.8717529184382807,0.02156...|  0.0|       0.0|\n+------------------------------+-------------+------------------------------+-----+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "print('-----------------------------Check Top 5 predictions----------------------------------')\n",
    "print(' ')\n",
    "predictions_cv_lr.select('Description','Category',\"probability\",\"label\",\"prediction\")\\\n",
    "                                        .orderBy(\"probability\", ascending=False)\\\n",
    "                                        .show(n=5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3faf720-5ea9-4aea-b45b-5a38490e843d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n------------------------------Accuracy----------------------------------\n \n                       accuracy:0.9723579882349168:\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "evaluator_cv_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_lr)\n",
    "print(' ')\n",
    "print('------------------------------Accuracy----------------------------------')\n",
    "print(' ')\n",
    "print('                       accuracy:{}:'.format(evaluator_cv_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c9299a-2c95-4666-abc0-e934c7d640d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n--------------------------Accuracy-----------------------------\n \n                      accuracy:0.9935325400900984:\n"
     ]
    }
   ],
   "source": [
    "### Secondary model using NaiveBayes\n",
    "pipeline_cv_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, nb])\n",
    "model_cv_nb = pipeline_cv_nb.fit(training)\n",
    "predictions_cv_nb = model_cv_nb.transform(test)\n",
    "evaluator_cv_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_nb)\n",
    "print(' ')\n",
    "print('--------------------------Accuracy-----------------------------')\n",
    "print(' ')\n",
    "print('                      accuracy:{}:'.format(evaluator_cv_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc005aa-e6ea-49a3-a4db-d6445c0fdc68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_idf_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,hashingTf, idf, label_string_idx, lr])\n",
    "model_idf_lr = pipeline_idf_lr.fit(training)\n",
    "predictions_idf_lr = model_idf_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b57375ff-afbb-443d-9da8-c70373eb1605",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Check Top 5 predictions----------------------------------\n \n+------------------------------+-------------+------------------------------+-----+----------+\n|                   Description|     Category|                   probability|label|prediction|\n+------------------------------+-------------+------------------------------+-----+----------+\n|theft, bicycle, <$50, no se...|larceny/theft|[0.8841834752091807,0.01952...|  0.0|       0.0|\n|theft, bicycle, <$50, no se...|larceny/theft|[0.8841834752091807,0.01952...|  0.0|       0.0|\n|theft, bicycle, <$50, no se...|larceny/theft|[0.8841834752091807,0.01952...|  0.0|       0.0|\n|theft, bicycle, <$50, no se...|larceny/theft|[0.8841834752091807,0.01952...|  0.0|       0.0|\n|theft, bicycle, <$50, no se...|larceny/theft|[0.8841834752091807,0.01952...|  0.0|       0.0|\n+------------------------------+-------------+------------------------------+-----+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "print('-----------------------------Check Top 5 predictions----------------------------------')\n",
    "print(' ')\n",
    "predictions_idf_lr.select('Description','Category',\"probability\",\"label\",\"prediction\")\\\n",
    "                                        .orderBy(\"probability\", ascending=False)\\\n",
    "                                        .show(n=5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f41cae8-fc00-4049-bbd3-f67a49d4930b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_idf_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,hashingTf, idf, label_string_idx, nb])\n",
    "model_idf_nb = pipeline_idf_nb.fit(training)\n",
    "predictions_idf_nb = model_idf_nb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "616d976e-87a0-44ee-b555-b5185c469267",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n-----------------------------Accuracy-----------------------------\n \n                          accuracy:0.9949302190940209:\n"
     ]
    }
   ],
   "source": [
    "evaluator_idf_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_idf_nb)\n",
    "print(' ')\n",
    "print('-----------------------------Accuracy-----------------------------')\n",
    "print(' ')\n",
    "print('                          accuracy:{}:'.format(evaluator_idf_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703048e3-6d0a-4e6e-b9ab-8856498f2845",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_wv_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover, word2Vec, label_string_idx, lr])\n",
    "model_wv_lr = pipeline_wv_lr.fit(training)\n",
    "predictions_wv_lr = model_wv_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60da03c6-0ceb-409e-abf8-75749f0f14e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator_wv_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_wv_lr)\n",
    "print('--------------------------Accuracy------------')\n",
    "print(' ')\n",
    "print('                  accuracy:{}:'.format(evaluator_wv_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b1aa619a-0816-4244-a519-105d246154bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pipeline_wv_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover, word2Vec, label_string_idx, nb])\n",
    "#model_wv_nb = pipeline_wv_nb.fit(training)\n",
    "#predictions_wv_nb = model_wv_nb.transform(test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b233383-d716-4ff3-b4d5-f069cbc034dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#evaluator_wv_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_wv_nb)\n",
    "#print('--------Accuracy------------')\n",
    "#print(' ')\n",
    "#print('accuracy:{}%:'.format(round(evaluator_wv_nb *100),2))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Classification-SanFrancisco Crime data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
