{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d47944-b3d2-4ec3-a362-6a149c3c2474",
   "metadata": {},
   "source": [
    "# Feature Extraction and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3b5fe6-369a-45f4-ab22-5f352f15933e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc11ae8-39b7-41e7-899c-9d9eef896e93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------+\n",
      "|id |text                                |\n",
      "+---+------------------------------------+\n",
      "|0  |This is the first document          |\n",
      "|1  |This document is the second document|\n",
      "|2  |And this is the third one           |\n",
      "|3  |Is this the first document?         |\n",
      "|4  |The last document is the fifth one  |\n",
      "+---+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer \n",
    "spark = SparkSession.builder.appName(\"TF-IDF Example\").getOrCreate() \n",
    "data = [ (0, \"This is the first document\"),   (1, \"This document is the second document\"), \n",
    "    (2, \"And this is the third one\"),   (3, \"Is this the first document?\"), \n",
    "    (4, \"The last document is the fifth one\") ] \n",
    "df = spark.createDataFrame(data, [\"id\", \"text\"]) \n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3969d738-1fc2-45b7-bca3-07ec90570d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------+--------------------------------------------------------------------------------------------------------+\n",
      "|id |text                                |features                                                                                                |\n",
      "+---+------------------------------------+--------------------------------------------------------------------------------------------------------+\n",
      "|0  |This is the first document          |(15,[2,3,4,10,13],[0.0,0.1823215567939546,0.0,0.4054651081081644,0.6931471805599453])                   |\n",
      "|1  |This document is the second document|(15,[2,3,4,6,10],[0.0,0.1823215567939546,0.0,0.6931471805599453,0.8109302162163288])                    |\n",
      "|2  |And this is the third one           |(15,[0,1,2,3,4,6],[0.6931471805599453,0.6931471805599453,0.0,0.1823215567939546,0.0,0.6931471805599453])|\n",
      "|3  |Is this the first document?         |(15,[1,2,3,4,13],[0.6931471805599453,0.0,0.1823215567939546,0.0,0.6931471805599453])                    |\n",
      "|4  |The last document is the fifth one  |(15,[0,2,4,9,10],[0.6931471805599453,0.0,0.0,1.0986122886681098,0.4054651081081644])                    |\n",
      "+---+------------------------------------+--------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(df) \n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=15) \n",
    "featurized_df = hashingTF.transform(words_df) \n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\") \n",
    "idf_model = idf.fit(featurized_df) \n",
    "tfidf_df = idf_model.transform(featurized_df) \n",
    "result_df = tfidf_df.select(\"id\", \"text\", \"features\") \n",
    "print(result_df.show(truncate=False,vertical=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430633fd-bebc-4f92-acc7-15e47edae63f",
   "metadata": {},
   "source": [
    "### Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "653d77e1-1576-400e-8125-00a3f50e8c09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n",
      "|id |words                         |\n",
      "+---+------------------------------+\n",
      "|0  |[apple, banana, orange, grape]|\n",
      "|1  |[apple, banana, cherry, pear] |\n",
      "|2  |[banana, cherry, grape, kiwi] |\n",
      "|3  |[apple, pear, kiwi, orange]   |\n",
      "|4  |[cherry, grape, kiwi, orange] |\n",
      "+---+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data (documents with tokenized words) \n",
    "from pyspark.ml.feature import Word2Vec, Tokenizer \n",
    "data = [     (0, [\"apple\", \"banana\", \"orange\", \"grape\"]),     (1, [\"apple\", \"banana\", \"cherry\", \"pear\"]), \n",
    "    (2, [\"banana\", \"cherry\", \"grape\", \"kiwi\"]),     (3, [\"apple\", \"pear\", \"kiwi\", \"orange\"]), \n",
    "   (4, [\"cherry\", \"grape\", \"kiwi\", \"orange\"]) ] \n",
    "# Create a DataFrame from the sample data \n",
    "df = spark.createDataFrame(data, [\"id\", \"words\"]) \n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff9afacb-e410-402a-838c-882dda28d9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------------------------+\n",
      "|id |features                                                         |\n",
      "+---+-----------------------------------------------------------------+\n",
      "|0  |[0.06720347004011273,0.051968781277537346,0.0366170909255743]    |\n",
      "|1  |[-8.117086254060268E-4,0.10535624250769615,-0.015570234507322311]|\n",
      "|2  |[-0.014742250088602304,0.021969905123114586,0.06884750723838806] |\n",
      "|3  |[0.06535391230136156,0.12924792431294918,-0.022874346002936363]  |\n",
      "|4  |[0.02541335765272379,0.046014076098799706,0.07840530946850777]   |\n",
      "+---+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"features\") \n",
    "model = word2vec.fit(df) \n",
    "result = model.transform(df) \n",
    "result.select(\"id\", \"features\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f294c6-13d6-478b-9e30-d5175622882e",
   "metadata": {},
   "source": [
    "### CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64fe0221-a7d3-4cca-83da-26026de399bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n",
      "|id |words                         |\n",
      "+---+------------------------------+\n",
      "|0  |[apple, banana, orange, grape]|\n",
      "|1  |[apple, banana, cherry, pear] |\n",
      "|2  |[banana, cherry, grape, kiwi] |\n",
      "|3  |[apple, pear, kiwi, orange]   |\n",
      "|4  |[cherry, grape, kiwi, orange] |\n",
      "+---+------------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data = [  (0, [\"apple\", \"banana\", \"orange\", \"grape\"]), (1, [\"apple\", \"banana\", \"cherry\", \"pear\"]), \n",
    "    (2, [\"banana\", \"cherry\", \"grape\", \"kiwi\"]), (3, [\"apple\", \"pear\", \"kiwi\", \"orange\"]), \n",
    "    (4, [\"cherry\", \"grape\", \"kiwi\", \"orange\"]) ] \n",
    "df = spark.createDataFrame(data, [\"id\", \"words\"]) \n",
    "print(df.show(truncate=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73ba7981-504b-4b6f-aeda-582f621b27d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------+\n",
      "|id |features                       |\n",
      "+---+-------------------------------+\n",
      "|0  |(7,[0,1,4,5],[1.0,1.0,1.0,1.0])|\n",
      "|1  |(7,[0,2,5,6],[1.0,1.0,1.0,1.0])|\n",
      "|2  |(7,[0,1,2,3],[1.0,1.0,1.0,1.0])|\n",
      "|3  |(7,[3,4,5,6],[1.0,1.0,1.0,1.0])|\n",
      "|4  |(7,[1,2,3,4],[1.0,1.0,1.0,1.0])|\n",
      "+---+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, Tokenizer \n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=7) \n",
    "model = cv.fit(df) \n",
    "result = model.transform(df) \n",
    "result.select(\"id\", \"features\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049435b-eb59-4cbb-b069-8a17c5178983",
   "metadata": {},
   "source": [
    "### FeatureHasher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4697652-25e2-4df1-9c03-77e03cd5a8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------+\n",
      "|id |feature1|feature2|feature3|\n",
      "+---+--------+--------+--------+\n",
      "|0  |apple   |banana  |orange  |\n",
      "|1  |apple   |banana  |cherry  |\n",
      "|2  |banana  |cherry  |grape   |\n",
      "|3  |apple   |pear    |kiwi    |\n",
      "|4  |cherry  |grape   |kiwi    |\n",
      "+---+--------+--------+--------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import FeatureHasher, Tokenizer \n",
    "spark = SparkSession.builder.appName(\"FeatureHasher Example\").getOrCreate() \n",
    "data = [ (0, \"apple\", \"banana\", \"orange\"), (1, \"apple\", \"banana\", \"cherry\"),  (2, \"banana\", \"cherry\", \"grape\"), \n",
    "    (3, \"apple\", \"pear\", \"kiwi\"),  (4, \"cherry\", \"grape\", \"kiwi\") ] \n",
    "df = spark.createDataFrame(data, [\"id\", \"feature1\", \"feature2\", \"feature3\"]) \n",
    "print(df.show(truncate=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca238254-20ec-4d45-9b2f-a216410343b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------+--------------------------+\n",
      "|id |feature1|feature2|feature3|features                  |\n",
      "+---+--------+--------+--------+--------------------------+\n",
      "|0  |apple   |banana  |orange  |(10,[3,7,9],[1.0,1.0,1.0])|\n",
      "|1  |apple   |banana  |cherry  |(10,[3,6,7],[1.0,1.0,1.0])|\n",
      "|2  |banana  |cherry  |grape   |(10,[0,4,7],[1.0,1.0,1.0])|\n",
      "|3  |apple   |pear    |kiwi    |(10,[0,7,8],[1.0,1.0,1.0])|\n",
      "|4  |cherry  |grape   |kiwi    |(10,[0,2,6],[1.0,1.0,1.0])|\n",
      "+---+--------+--------+--------+--------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hasher = FeatureHasher(inputCols=[\"feature1\", \"feature2\", \"feature3\"],outputCol=\"features\", numFeatures=10) \n",
    "result = hasher.transform(df) \n",
    "print(result.show(truncate=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ec262-09d5-4baa-8e38-50924892b0ee",
   "metadata": {},
   "source": [
    "### VectorAssembler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b83b4e2-3844-4c45-8585-70fa31caab8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+\n",
      "|feature1|feature2|feature3|feature4|\n",
      "+--------+--------+--------+--------+\n",
      "|       1|       2|       3|       4|\n",
      "|       5|       6|       7|       8|\n",
      "|       9|      10|      11|      12|\n",
      "+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler \n",
    "data = [ (1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12) ] \n",
    "columns = [\"feature1\", \"feature2\", \"feature3\", \"feature4\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a05319a-8843-48fc-a5f7-078d233ff9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------------------+\n",
      "|feature1|feature2|feature3|feature4|            features|\n",
      "+--------+--------+--------+--------+--------------------+\n",
      "|       1|       2|       3|       4|   [1.0,2.0,3.0,4.0]|\n",
      "|       5|       6|       7|       8|   [5.0,6.0,7.0,8.0]|\n",
      "|       9|      10|      11|      12|[9.0,10.0,11.0,12.0]|\n",
      "+--------+--------+--------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\") \n",
    "output_df = assembler.transform(df) \n",
    "output_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abe7cf-2212-4a11-bd2f-0bda05e14637",
   "metadata": {},
   "source": [
    "### StringIndexer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dd9b4d3-860b-4309-a22b-c55fa733ce22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|Categories|Value|\n",
      "+----------+-----+\n",
      "|         A|   10|\n",
      "|         A|   20|\n",
      "|         B|   30|\n",
      "|         B|   20|\n",
      "|         B|   30|\n",
      "|         C|   40|\n",
      "|         C|   10|\n",
      "|         D|   10|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer \n",
    "data = [(\"A\", 10), (\"A\", 20), (\"B\", 30), (\"B\", 20), (\"B\", 30), (\"C\", 40), (\"C\", 10), (\"D\", 10)] \n",
    "columns = [\"Categories\", \"Value\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e60b400e-5157-4c55-8bd4-de4f9fc6aea6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+\n",
      "|Categories|Value|Categories_Indexed|\n",
      "+----------+-----+------------------+\n",
      "|         A|   10|               1.0|\n",
      "|         A|   20|               1.0|\n",
      "|         B|   30|               0.0|\n",
      "|         B|   20|               0.0|\n",
      "|         B|   30|               0.0|\n",
      "|         C|   40|               2.0|\n",
      "|         C|   10|               2.0|\n",
      "|         D|   10|               3.0|\n",
      "+----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"Categories_Indexed\") \n",
    "indexerModel = indexer.fit(df) \n",
    "indexed_df = indexerModel.transform(df) \n",
    "indexed_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839317b1-996c-4a1b-82f6-88a5a2a67216",
   "metadata": {},
   "source": [
    "### OneHotEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ecf2df4-94e9-477c-810c-1e5fe937f21c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|input1|input2|\n",
      "+------+------+\n",
      "|   0.0|   1.0|\n",
      "|   1.0|   0.0|\n",
      "|   2.0|   1.0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder \n",
    "data = [(0.0, 1.0), (1.0, 0.0), (2.0, 1.0)] \n",
    "columns = [\"input1\", \"input2\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c981e44-9f84-44f4-ada0-4950aeb82462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|output1      |output2      |\n",
      "+-------------+-------------+\n",
      "|(2,[0],[1.0])|(1,[],[])    |\n",
      "|(2,[1],[1.0])|(1,[0],[1.0])|\n",
      "|(2,[],[])    |(1,[],[])    |\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(inputCols=[\"input1\", \"input2\"], outputCols=[\"output1\", \"output2\"]) \n",
    "encoded_df = encoder.fit(df) \n",
    "encoded_df = encoded_df.transform(df) \n",
    "encoded_df.select(\"output1\",\"output2\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d1653-b372-48b5-9655-afa4ec8bb063",
   "metadata": {},
   "source": [
    "### Tokenizer and RegexTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25bbc082-5b72-459e-98d7-e7c06a1025fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Th+is is a sam+pl...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer \n",
    "spark = SparkSession.builder.appName(\"TokenizerExample\").getOrCreate() \n",
    "data = [(\"Th+is is a sam+ple sent+ence.\",)] \n",
    "columns = [\"text\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd12caac-4fcb-4a5e-b45d-63e9fc07398b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|tokens                             |\n",
      "+-----------------------------------+\n",
      "|[th+is, is, a, sam+ple, sent+ence.]|\n",
      "+-----------------------------------+\n",
      "\n",
      "+----------------------------------+\n",
      "|regex_tokens                      |\n",
      "+----------------------------------+\n",
      "|[th, is is a sam, ple sent, ence.]|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\") \n",
    "tokenized_df = tokenizer.transform(df) \n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"regex_tokens\", pattern=\"\\\\+\") \n",
    "regex_tokenized_df = regex_tokenizer.transform(df) \n",
    "tokenized_df.select(\"tokens\").show(truncate=False) \n",
    "regex_tokenized_df.select(\"regex_tokens\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9e7a0-4d9e-40c6-a26b-779eeb5335ff",
   "metadata": {},
   "source": [
    "### StopWordsRemover "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2128b1dd-e86d-4f8f-8d66-a3d44bae3b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|text                               |words                                     |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|This is the first sentence.        |[this, is, the, first, sentence.]         |\n",
      "|And here's another sentence.       |[and, here's, another, sentence.]         |\n",
      "|A third sentence for the DataFrame.|[a, third, sentence, for, the, dataframe.]|\n",
      "+-----------------------------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer \n",
    "data = [(\"This is the first sentence.\",), (\"And here's another sentence.\",),  (\"A third sentence for the DataFrame.\",)] \n",
    "columns = [\"text\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") \n",
    "df = tokenizer.transform(df) \n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b40bf4f-bc52-43ed-94b2-a37850e94b33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|filtered_words               |\n",
      "+-----------------------------+\n",
      "|[first, sentence.]           |\n",
      "|[another, sentence.]         |\n",
      "|[third, sentence, dataframe.]|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\") \n",
    "filtered_df = stopwords_remover.transform(df) \n",
    "filtered_df.select(\"filtered_words\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6005c-9e18-4c57-8b45-3d8d19a44a3c",
   "metadata": {},
   "source": [
    "### Bucketizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72a9c69e-9a83-4704-b357-e8a980248883",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  0|  1.5|\n",
      "|  1|  2.5|\n",
      "|  2|  3.5|\n",
      "|  3|  4.5|\n",
      "|  4|  5.5|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import Bucketizer \n",
    "from pyspark.sql.functions import col \n",
    "data = [(0, 1.5), (1, 2.5), (2, 3.5), (3, 4.5), (4, 5.5)] \n",
    "columns = [\"id\", \"value\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b81a776-21ab-4250-9088-5e8b0ca663c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id|value|bucket|\n",
      "+---+-----+------+\n",
      "|  0|  1.5|   0.0|\n",
      "|  1|  2.5|   1.0|\n",
      "|  2|  3.5|   1.0|\n",
      "|  3|  4.5|   2.0|\n",
      "|  4|  5.5|   2.0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = [0.0, 2.0, 4.0, float(\"inf\")] \n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"value\", outputCol=\"bucket\") \n",
    "bucketized_df = bucketizer.transform(df) \n",
    "bucketized_df.select(\"id\", \"value\", \"bucket\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2bbb7-62a8-4f56-8bef-1239d9000e0f",
   "metadata": {},
   "source": [
    "### StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "737d3dc6-0d70-4fac-a3d2-ffe5518b9658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  2|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import StandardScaler \n",
    "from pyspark.ml.linalg import Vectors \n",
    "data = [(0, Vectors.dense([1.0, 0.1, -1.0]),), (1, Vectors.dense([2.0, 1.1, 1.0]),), \n",
    "        (2, Vectors.dense([3.0, 10.1, 3.0]),)] \n",
    "columns = [\"id\", \"features\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb9565f4-1075-48bc-9b87-6204e53c8e82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------+\n",
      "|id |features      |scaled_features                |\n",
      "+---+--------------+-------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[-1.0,-0.6657502859356826,-1.0]|\n",
      "|1  |[2.0,1.1,1.0] |[0.0,-0.4841820261350419,0.0]  |\n",
      "|2  |[3.0,10.1,3.0]|[1.0,1.1499323120707245,1.0]   |\n",
      "+---+--------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True) \n",
    "scaler_model = scaler.fit(df) \n",
    "scaled_df = scaler_model.transform(df) \n",
    "scaled_df.select(\"id\", \"features\", \"scaled_features\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38143d0-438c-4a5a-bab8-362a4b815b9a",
   "metadata": {},
   "source": [
    "### MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37215196-daf3-423e-9ae3-acfe26b008a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  2|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import MinMaxScaler \n",
    "from pyspark.ml.linalg import Vectors \n",
    "data = [(0, Vectors.dense([1.0, 0.1, -1.0]),), \n",
    "        (1, Vectors.dense([2.0, 1.1, 1.0]),), \n",
    "        (2, Vectors.dense([3.0, 10.1, 3.0]),)] \n",
    "columns = [\"id\", \"features\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad4813dd-45b9-451e-b3a3-005143c700bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------+\n",
      "|id |features      |scaled_features|\n",
      "+---+--------------+---------------+\n",
      "|0  |[1.0,0.1,-1.0]|(3,[],[])      |\n",
      "|1  |[2.0,1.1,1.0] |[0.5,0.1,0.5]  |\n",
      "|2  |[3.0,10.1,3.0]|[1.0,1.0,1.0]  |\n",
      "+---+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\") \n",
    "scaler_model = scaler.fit(df) \n",
    "scaled_df = scaler_model.transform(df) \n",
    "scaled_df.select(\"id\", \"features\", \"scaled_features\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1277e2-acd6-4c31-8757-81a37210eafe",
   "metadata": {},
   "source": [
    "### Normalizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2177963f-ad93-4283-ab95-de8efaf6fa95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  2|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import Normalizer \n",
    "from pyspark.ml.linalg import Vectors \n",
    "data = [(0, Vectors.dense([1.0, 0.1, -1.0]),), \n",
    "        (1, Vectors.dense([2.0, 1.1, 1.0]),), \n",
    "        (2, Vectors.dense([3.0, 10.1, 3.0]),)] \n",
    "columns = [\"id\", \"features\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5a760c9-5cef-473c-8925-b986e2ffaf32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------------------------+\n",
      "|id |features      |normalized_features                                         |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.7053456158585983,0.07053456158585983,-0.7053456158585983]|\n",
      "|1  |[2.0,1.1,1.0] |[0.8025723539051279,0.4414147946478204,0.40128617695256397] |\n",
      "|2  |[3.0,10.1,3.0]|[0.27384986857909926,0.9219612242163009,0.27384986857909926]|\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normalized_features\", p=2.0) \n",
    "normalized_df = normalizer.transform(df) \n",
    "normalized_df.select(\"id\", \"features\", \"normalized_features\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb31beb-ebd7-4d17-ad6d-3f01b4834b06",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e15c5957-9442-47d9-8cb0-2b94976f34ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  2|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import PCA \n",
    "from pyspark.ml.linalg import Vectors \n",
    "data = [(0, Vectors.dense([1.0, 0.1, -1.0])), \n",
    "        (1, Vectors.dense([2.0, 1.1, 1.0])), \n",
    "        (2, Vectors.dense([3.0, 10.1, 3.0]))] \n",
    "columns = [\"id\", \"features\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fda2890b-bea4-4bf9-8aef-155845bfcd29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------+\n",
      "|id |pca_features                              |\n",
      "+---+------------------------------------------+\n",
      "|0  |[0.06466700238304013,-0.45367188451874657]|\n",
      "|1  |[-1.6616789696362084,1.284065030233573]   |\n",
      "|2  |[-10.870750062210382,0.19181523649833387] |\n",
      "+---+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\") \n",
    "pca_model = pca.fit(df) \n",
    "pca_df = pca_model.transform(df) \n",
    "pca_df.select(\"id\", \"pca_features\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3f22e-0a43-4daf-a0f9-cb1afa8062ad",
   "metadata": {},
   "source": [
    "### PolynomialExpansion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfc20e5f-505b-4e63-af09-37f52e4a720f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id| features|\n",
      "+---+---------+\n",
      "|  0|[1.0,2.0]|\n",
      "|  1|[2.0,3.0]|\n",
      "|  2|[3.0,4.0]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import PolynomialExpansion \n",
    "from pyspark.ml.linalg import Vectors \n",
    "data = [(0, Vectors.dense([1.0, 2.0])), (1, Vectors.dense([2.0, 3.0])), (2, Vectors.dense([3.0, 4.0]))] \n",
    "columns = [\"id\", \"features\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d0ab9c4-8f29-4d86-b415-3136d0bd71e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |expanded_features      |\n",
      "+---+-----------------------+\n",
      "|0  |[1.0,1.0,2.0,2.0,4.0]  |\n",
      "|1  |[2.0,4.0,3.0,6.0,9.0]  |\n",
      "|2  |[3.0,9.0,4.0,12.0,16.0]|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poly_expansion = PolynomialExpansion(inputCol=\"features\", outputCol=\"expanded_features\", degree=2) \n",
    "expanded_df = poly_expansion.transform(df) \n",
    "expanded_df.select(\"id\", \"expanded_features\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf556aa-2a09-4db3-8d3d-f9e0e3c8aefc",
   "metadata": {},
   "source": [
    "### Chi-Squared Selector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9ea8b79-4906-43f4-8f71-5131604a6a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----+\n",
      "| id|      features|label|\n",
      "+---+--------------+-----+\n",
      "|  0|[1.0,0.1,-1.0]|  1.0|\n",
      "|  1| [2.0,1.1,1.0]|  0.0|\n",
      "|  2|[3.0,10.1,3.0]|  0.0|\n",
      "+---+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import ChiSqSelector \n",
    "from pyspark.ml.linalg import Vectors \n",
    "from pyspark.sql.functions import col \n",
    "data = [(0, Vectors.dense([1.0, 0.1, -1.0]), 1.0), \n",
    "        (1, Vectors.dense([2.0, 1.1, 1.0]), 0.0), \n",
    "        (2, Vectors.dense([3.0, 10.1, 3.0]), 0.0)] \n",
    "columns = [\"id\", \"features\", \"label\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1338ce1-da4e-4731-9385-13cc6172f473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "|id |selected_features|\n",
      "+---+-----------------+\n",
      "|0  |[1.0]            |\n",
      "|1  |[2.0]            |\n",
      "|2  |[3.0]            |\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\", outputCol=\"selected_features\", labelCol=\"label\") \n",
    "selector_model = selector.fit(df) \n",
    "selected_df = selector_model.transform(df) \n",
    "selected_df.select(\"id\", \"selected_features\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1c86f-a91c-4fe7-b81e-c930e6f9747e",
   "metadata": {},
   "source": [
    "### Vector Slicer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5ff15dc-5eb0-4e7f-9bc8-02301f0c5ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|[1.0,2.0,3.0,4.0,...|\n",
      "|  1|[2.0,3.0,4.0,5.0,...|\n",
      "|  2|[3.0,4.0,5.0,6.0,...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import VectorSlicer \n",
    "from pyspark.ml.linalg import Vectors \n",
    "data = [(0, Vectors.dense([1.0, 2.0, 3.0, 4.0, 5.0])), \n",
    "        (1, Vectors.dense([2.0, 3.0, 4.0, 5.0, 6.0])), \n",
    "        (2, Vectors.dense([3.0, 4.0, 5.0, 6.0, 7.0]))] \n",
    "columns = [\"id\", \"features\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3cc9fea-f735-49b4-9283-b0e8f8dbdba9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "|id |selected_features|\n",
      "+---+-----------------+\n",
      "|0  |[2.0,4.0,5.0]    |\n",
      "|1  |[3.0,5.0,6.0]    |\n",
      "|2  |[4.0,6.0,7.0]    |\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slicer = VectorSlicer(inputCol=\"features\", outputCol=\"selected_features\", indices=[1, 3, 4]) \n",
    "sliced_df = slicer.transform(df) \n",
    "sliced_df.select(\"id\", \"selected_features\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722bdb43-3f36-444b-a377-e86452e25f88",
   "metadata": {},
   "source": [
    "### RFormula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "499ee12c-546a-4e85-94a9-efc798485525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  y|  x|  s|\n",
      "+---+---+---+\n",
      "|1.0|1.0|  a|\n",
      "|0.0|2.0|  b|\n",
      "|0.0|0.0|  a|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import RFormula \n",
    "spark = SparkSession.builder.appName(\"RFormulaExample\").getOrCreate() \n",
    "data = [(1.0, 1.0, \"a\"), \n",
    "        (0.0, 2.0, \"b\"), \n",
    "        (0.0, 0.0, \"a\")] \n",
    "columns = [\"y\", \"x\", \"s\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2367181a-6393-4d5f-86d7-86d3f3e1dd70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---------+-----+\n",
      "|y  |x  |s  |features |label|\n",
      "+---+---+---+---------+-----+\n",
      "|1.0|1.0|a  |[1.0,1.0]|1.0  |\n",
      "|0.0|2.0|b  |[2.0,0.0]|0.0  |\n",
      "|0.0|0.0|a  |[0.0,1.0]|0.0  |\n",
      "+---+---+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RFormula(formula=\"y ~ x + s\") \n",
    "model = rf.fit(df) \n",
    "transformed_df = model.transform(df) \n",
    "transformed_df.select(\"y\", \"x\", \"s\", \"features\", \"label\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647c2f7-30aa-4fed-bbb0-bfba7f8388ee",
   "metadata": {},
   "source": [
    "### UnivariateFeatureSelector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d95dc162-4c69-4e47-a93e-e6ab09476522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|  1.0|[1.0,0.1,-1.0]|\n",
      "|  0.0| [2.0,1.1,1.0]|\n",
      "|  0.0|[3.0,10.1,3.0]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import UnivariateFeatureSelector \n",
    "from pyspark.ml.linalg import Vectors \n",
    "data = [(1.0, Vectors.dense([1.0, 0.1, -1.0])), \n",
    "        (0.0, Vectors.dense([2.0, 1.1, 1.0])), \n",
    "        (0.0, Vectors.dense([3.0, 10.1, 3.0]))] \n",
    "columns = [\"label\", \"features\"] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3c01d36-cc61-4a3d-9fa0-6b4373f9b136",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|selected_features|\n",
      "+-----+-----------------+\n",
      "|1.0  |[1.0]            |\n",
      "|0.0  |[2.0]            |\n",
      "|0.0  |[3.0]            |\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selector = UnivariateFeatureSelector(featuresCol=\"features\", outputCol=\"selected_features\") \n",
    "selector.setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(1) \n",
    "selected_df = selector.fit(df).transform(df) \n",
    "selected_df.select(\"label\", \"selected_features\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf58e2c-2d57-42a2-b206-6011cf5596d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(hard-disk) Python",
   "language": "python",
   "name": "conda-env-hard-disk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
