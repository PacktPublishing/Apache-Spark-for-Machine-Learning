{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cde3ede-0eaa-4881-bdb5-7a363ed11afb",
   "metadata": {},
   "source": [
    "# Clustering with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b884561-a265-4d7b-869a-8915fdb74e91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "106c3f54-7a33-4ac6-a637-c7d5ca2081b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer,CountVectorizer,IDF\n",
    "import re\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"NewsgroupsPreprocessing\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "num_features = 8000  #vocabulary size\n",
    "num_topics = 20      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92a91d-4191-4acd-a723-23d5cda99718",
   "metadata": {},
   "source": [
    "### Load the 20 Newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87e6ceb2-584e-4735-8068-a0ca3b8df23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|doc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "# Convert the text data to a DataFrame\n",
    "df = spark.createDataFrame([(doc,) for doc in newsgroups.data], [\"doc\"])\n",
    "df.show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc54660-87e1-4517-9745-dc3f645efcb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing the data for LDA to remove non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53bbd867-4f48-4ea6-887c-8007fede7a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|cleaned_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  I am sure some bashers of Pens fans are pretty confused about the lack of any kind of posts about the recent Pens massacre of the Devils. Actually, I am  bit puzzled too and a bit relieved. However, I am going to put an end to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they are killing those Devils worse than I thought. Jagr just showed you why he is much better than his regular season stats. He is also a lot fo fun to watch in the playoffs. Bowman should let JAgr have a lot of fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final regular season game.          PENS RULE!!!  |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "# Define a UDF to apply regex and remove non-alphabetic characters\n",
    "def clean_text(doc):\n",
    "    cleaned_doc = re.sub(r\"[^A-Za-z]\", \" \", doc)\n",
    "    return \" \".join(cleaned_doc.split())  # Remove extra spaces\n",
    "\n",
    "# Register the UDF\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Apply regex \n",
    "df_cleaned = df.withColumn(\"cleaned_doc\", clean_text_udf(\"doc\"))\n",
    "# Replace newline characters with a space\n",
    "df_cleaned = df.withColumn(\"cleaned_text\", regexp_replace(\"doc\", \"\\n\", \" \"))\n",
    "df_cleaned.select(\"cleaned_text\").show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02ccec-2950-4585-899d-875356e52c88",
   "metadata": {},
   "source": [
    "### Tokenize,remove stop words and count vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ea79dcd-fcd8-4d54-bac5-c432b5ee6751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|filtered_array                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[sure, bashers, pens, fans, pretty, confused, lack, kind, posts, recent, pens, massacre, devils., actually,, puzzled, relieved., however,, going, non-pittsburghers', relief, praise, pens., man,, killing, devils, worse, thought., jagr, showed, much, better, regular, season, stats., also, watch, playoffs., bowman, jagr, next, couple, games, since, pens, going, beat, pulp, jersey, anyway., disappointed, islanders, lose, final, regular, season, game., pens, rule!!!]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Apply Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\")\n",
    "df_tokenized = tokenizer.transform(df_cleaned)\n",
    "\n",
    "# Apply StopWordsRemover\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_doc\")\n",
    "df_filtered = stopwords_remover.transform(df_tokenized).select(\"filtered_doc\")\n",
    "\n",
    "\n",
    "\n",
    "# Filter array elements with at least 4 characters\n",
    "df_filtered = df_filtered.withColumn(\"filtered_array\",expr(\"filter(filtered_doc, x -> len(x) >= 4)\"))\n",
    "df_filtered = df_filtered.select(\"filtered_array\")\n",
    "df_filtered.show(1,truncate=False)\n",
    "\n",
    "# Apply CountVectorizer\n",
    "count_vec = CountVectorizer(inputCol=\"filtered_array\", outputCol=\"count_vec\" ,vocabSize=num_features, minDF=2.0)\n",
    "\n",
    "count_vec_model = count_vec.fit(df_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5eae5-30d9-4da3-b72f-f7143c4a4355",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Apply IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5c0bd6b-9d96-4ae1-8ebe-f8ab5a80b64f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vocab = count_vec_model.vocabulary\n",
    "newsgroups = count_vec_model.transform(df_filtered)\n",
    "#newsgroups = newsgroups.drop('filtered_array')\n",
    "    \n",
    "\n",
    "# Apply IDF\n",
    "idf = IDF(inputCol=\"count_vec\", outputCol=\"features\")\n",
    "newsgroups = idf.fit(newsgroups).transform(newsgroups)\n",
    "newsgroups = newsgroups.drop('tf_features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74681d-3368-43df-a751-5c3494e26834",
   "metadata": {},
   "source": [
    "### Apply LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5410341c-42b7-4996-bafb-509a0a4c9549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|      filtered_array|           count_vec|            features|   topicDistribution|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|[sure, bashers, p...|(8000,[3,10,17,20...|(8000,[3,10,17,20...|[2.16703122129341...|\n",
      "|[brother, market,...|(8000,[16,25,132,...|(8000,[16,25,132,...|[4.53226407567457...|\n",
      "|[finally, said, d...|(8000,[0,2,17,19,...|(8000,[0,2,17,19,...|[1.38682346408836...|\n",
      "|[think!, scsi, ca...|(8000,[13,49,83,8...|(8000,[13,49,83,8...|[2.05397425634834...|\n",
      "|[jasmine, drive, ...|(8000,[2,13,16,23...|(8000,[2,13,16,23...|[3.28641722820767...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[7, 756, 215, 344...|[0.11837926504713...|\n",
      "|    1|[159, 271, 297, 2...|[0.00924404670192...|\n",
      "|    2|[164, 44, 96, 277...|[0.00664370986381...|\n",
      "|    3|[1371, 105, 39, 8...|[0.00490986173629...|\n",
      "|    4|[152, 199, 612, 5...|[0.00963063064763...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA(k=num_topics, featuresCol=\"features\", seed=0)\n",
    "model = lda.fit(newsgroups)\n",
    "transformed_data = model.transform(newsgroups)\n",
    "transformed_data.show(5)\n",
    "\n",
    "topics = model.describeTopics()\n",
    "topics.show(5)\n",
    "\n",
    "model.topicsMatrix()\n",
    "    \n",
    "topics_rdd = topics.rdd\n",
    "\n",
    "topics_words = topics_rdd\\\n",
    "    .map(lambda row: row['termIndices'])\\\n",
    "    .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba1746-f984-4511-9477-fcf865e9014e",
   "metadata": {},
   "source": [
    "### Showing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6402729-231b-4da6-9cce-591a46d8312c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic:  0\n",
      "----------\n",
      "max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\n",
      "myers:\n",
      "president\n",
      "health\n",
      "administration\n",
      "think\n",
      "congress\n",
      "going\n",
      "jobs\n",
      "white\n",
      "----------\n",
      "topic:  1\n",
      "----------\n",
      "armenian\n",
      "armenians\n",
      "jews\n",
      "turkish\n",
      "people\n",
      "rights\n",
      "jewish\n",
      "israeli\n",
      "right\n",
      "government\n",
      "----------\n",
      "topic:  2\n",
      "----------\n",
      "window\n",
      "available\n",
      "version\n",
      "server\n",
      "file\n",
      "files\n",
      "anonymous\n",
      "widget\n",
      "motif\n",
      "windows\n",
      "----------\n",
      "topic:  3\n",
      "----------\n",
      "vitamin\n",
      "year\n",
      "number\n",
      "baseball\n",
      "play\n",
      "medical\n",
      "year.\n",
      "last\n",
      "players\n",
      "games\n",
      "----------\n",
      "topic:  4\n",
      "----------\n",
      "university\n",
      "1993\n",
      "greek\n",
      "page\n",
      "professor\n",
      "objective\n",
      "april\n",
      "science\n",
      "conference\n",
      "space\n",
      "----------\n",
      "topic:  5\n",
      "----------\n",
      "israel\n",
      "good\n",
      "israeli\n",
      "excellent\n",
      "great\n",
      "soldiers\n",
      "playing\n",
      "missing\n",
      "arabs\n",
      "cubs\n",
      "----------\n",
      "topic:  6\n",
      "----------\n",
      "file\n",
      "data\n",
      "entry\n",
      "bits\n",
      "code\n",
      "char\n",
      "source\n",
      "oname,\n",
      "output\n",
      "files\n",
      "----------\n",
      "topic:  7\n",
      "----------\n",
      "subject:\n",
      "message\n",
      "theory\n",
      "know\n",
      "make\n",
      "lines\n",
      "computer\n",
      "like\n",
      "science\n",
      "using\n",
      "----------\n",
      "topic:  8\n",
      "----------\n",
      "image\n",
      "images\n",
      "processing\n",
      "graphics\n",
      "data\n",
      "batf\n",
      "address\n",
      "format\n",
      "formats\n",
      "contact:\n",
      "----------\n",
      "topic:  9\n",
      "----------\n",
      "55.0\n",
      "team\n",
      "----\n",
      "hockey\n",
      "game\n",
      "(1st\n",
      "period\n",
      "play:\n",
      "scorer\n",
      "bullock\n",
      "----------\n",
      "topic:  10\n",
      "----------\n",
      "jesus\n",
      "people\n",
      "believe\n",
      "jpeg\n",
      "bible\n",
      "christian\n",
      "think\n",
      "christ\n",
      "many\n",
      "church\n",
      "----------\n",
      "topic:  11\n",
      "----------\n",
      "police\n",
      "said\n",
      "government\n",
      "even\n",
      "people\n",
      "children\n",
      "guns\n",
      "years\n",
      "went\n",
      "came\n",
      "----------\n",
      "topic:  12\n",
      "----------\n",
      "food\n",
      "controller\n",
      "speech\n",
      "(istanbul,\n",
      "stereo\n",
      "atari\n",
      "interested.\n",
      "chinese\n",
      "asking\n",
      "plug\n",
      "----------\n",
      "topic:  13\n",
      "----------\n",
      "space\n",
      "cancer\n",
      "research\n",
      "nasa\n",
      "center\n",
      "disease\n",
      "medical\n",
      "launch\n",
      "(pt)\n",
      "information\n",
      "----------\n",
      "topic:  14\n",
      "----------\n",
      "list\n",
      "1.00\n",
      "engine\n",
      "appears)\n",
      "please\n",
      "water\n",
      "company\n",
      "miles\n",
      "bike\n",
      "mailing\n",
      "----------\n",
      "topic:  15\n",
      "----------\n",
      "wire\n",
      "ground\n",
      "wiring\n",
      "neutral\n",
      "baltimore\n",
      "grounding\n",
      "john's\n",
      "circuit\n",
      "outlets\n",
      "electrical\n",
      "----------\n",
      "topic:  16\n",
      "----------\n",
      "insurance\n",
      "radar\n",
      "jews\n",
      "nazi\n",
      "male\n",
      "adaptec\n",
      "agents\n",
      "ripem\n",
      "detector\n",
      "state\n",
      "----------\n",
      "topic:  17\n",
      "----------\n",
      "encryption\n",
      "privacy\n",
      "government\n",
      "clipper\n",
      "homosexual\n",
      "public\n",
      "crypto\n",
      "keys\n",
      "technology\n",
      "chip\n",
      "----------\n",
      "topic:  18\n",
      "----------\n",
      "drive\n",
      "windows\n",
      "disk\n",
      "card\n",
      "system\n",
      "scsi\n",
      "hard\n",
      "like\n",
      "thanks\n",
      "anyone\n",
      "----------\n",
      "topic:  19\n",
      "----------\n",
      "firearms\n",
      "rate\n",
      "m\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\n",
      "[(*]\n",
      "firearm\n",
      "gamma\n",
      "alcohol\n",
      "pit-6\n",
      "shaft\n",
      "failure\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(topics_words):\n",
    "    print (\"topic: \", idx)\n",
    "    print (\"----------\")\n",
    "    for word in topic:\n",
    "        print (word)  # word\n",
    "    print (\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c8166-0c35-439b-9785-373353817cfe",
   "metadata": {},
   "source": [
    "### Visualize the clusters (using the first two components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ed3af-a4a5-479d-a395-01094699f603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Visualize the clusters (using the first two components)\n",
    "import matplotlib.pyplot as plt\n",
    "topics = model.describeTopics(10)\n",
    "topic_words = topics.select(\"termIndices\").rdd.flatMap(lambda x: x[0]).collect()\n",
    "vocab = count_vec_model.vocabulary\n",
    "topic_words_list = [[vocab[i] for i in [indices]] for indices in topic_words]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, words in enumerate(topic_words_list):\n",
    "    plt.scatter(transformed_data.select(\"topicDistribution\").collect()[i][0][0],\n",
    "                transformed_data.select(\"topicDistribution\").collect()[i][0][1],\n",
    "                label=f\"Topic {i}\", s=100)\n",
    "    for word in words:\n",
    "        plt.annotate(word, (transformed_data.select(\"topicDistribution\").collect()[i][0][0],\n",
    "                            transformed_data.select(\"topicDistribution\").collect()[i][0][1]),\n",
    "                     textcoords=\"offset points\", xytext=(0, 5), ha=\"center\")\n",
    "\n",
    "plt.title(\"LDA Clustering (20-Newsgroups)\")\n",
    "plt.xlabel(\"Topic 0\")\n",
    "plt.ylabel(\"Topic 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(hard-disk) Python",
   "language": "python",
   "name": "conda-env-hard-disk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
