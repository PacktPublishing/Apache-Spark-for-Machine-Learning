{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287340cd-3a91-4355-bdd5-882f18700d3a",
   "metadata": {},
   "source": [
    "# Windowing in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65a113e0-b478-4e81-a25c-3c7586c6778e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|dept_id|emp_id|salary|\n",
      "+-------+------+------+\n",
      "|  sales|     1|  7000|\n",
      "|  sales|     2|  6000|\n",
      "|  sales|     9|  5000|\n",
      "|  sales|    10|  6000|\n",
      "|     hr|     3|  2000|\n",
      "|     hr|     4|  6000|\n",
      "|     hr|     7|  3000|\n",
      "|     it|     5|  5000|\n",
      "|     it|     6| 11000|\n",
      "|     it|     8| 19000|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"GMMClustering\").getOrCreate()\n",
    "df = spark.createDataFrame(\n",
    " [(\"sales\",1,7000),(\"sales\",2,6000),(\"sales\",9,5000),(\"sales\",10,6000),(\"hr\",3,2000),(\"hr\",4,6000),(\"hr\",7,3000),(\"it\",5,5000),(\"it\",6,11000),(\"it\",8,19000)],[\"dept_id\",\"emp_id\",\"salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3afba-373d-48af-a30c-91f396682872",
   "metadata": {},
   "source": [
    "### Problem statement: How to calculate a new column for each group whose row value is equal to sum of the current row and previous 2 rows?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f58bbb3b-ba27-4790-be04-2cc97651354b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-----+\n",
      "|dept_id|emp_id|salary|  sum|\n",
      "+-------+------+------+-----+\n",
      "|     hr|     3|  2000| 2000|\n",
      "|     hr|     4|  6000| 8000|\n",
      "|     hr|     7|  3000|11000|\n",
      "|     it|     5|  5000| 5000|\n",
      "|     it|     6| 11000|16000|\n",
      "|     it|     8| 19000|35000|\n",
      "|  sales|     1|  7000| 7000|\n",
      "|  sales|     2|  6000|13000|\n",
      "|  sales|     9|  5000|18000|\n",
      "|  sales|    10|  6000|17000|\n",
      "+-------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import Window\n",
    "window = Window.partitionBy(\"dept_id\").orderBy(\"emp_id\").rowsBetween(-2, 0)\n",
    "df.withColumn(\"sum\",func.sum(\"salary\").over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b50d5e9e-e244-4e1e-b15f-ab4686e8a5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-----+\n",
      "|dept_id|emp_id|salary|  sum|\n",
      "+-------+------+------+-----+\n",
      "|     hr|     3|  2000|11000|\n",
      "|     hr|     4|  6000| 9000|\n",
      "|     hr|     7|  3000| 3000|\n",
      "|     it|     5|  5000|35000|\n",
      "|     it|     6| 11000|30000|\n",
      "|     it|     8| 19000|19000|\n",
      "|  sales|     1|  7000|18000|\n",
      "|  sales|     2|  6000|17000|\n",
      "|  sales|     9|  5000|11000|\n",
      "|  sales|    10|  6000| 6000|\n",
      "+-------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(\"emp_id\").rowsBetween(0, 2)\n",
    "df.withColumn(\"sum\",func.sum(\"salary\").over(window)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf3768-eb6b-4203-9a82-37556cf7b66a",
   "metadata": {},
   "source": [
    "### How to calculate the lag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e755c90-9d50-4176-b015-56025e21ab42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------------------+\n",
      "|dept_id|emp_id|salary|previousrow_salary|\n",
      "+-------+------+------+------------------+\n",
      "|     hr|     4|  6000|              NULL|\n",
      "|     hr|     7|  3000|              6000|\n",
      "|     hr|     3|  2000|              3000|\n",
      "|     it|     8| 19000|              NULL|\n",
      "|     it|     6| 11000|             19000|\n",
      "|     it|     5|  5000|             11000|\n",
      "|  sales|     1|  7000|              NULL|\n",
      "|  sales|     2|  6000|              7000|\n",
      "|  sales|    10|  6000|              6000|\n",
      "|  sales|     9|  5000|              6000|\n",
      "+-------+------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(func.col(\"salary\").desc())\n",
    "df.withColumn(\"previousrow_salary\",func.lag('salary',1).over(window)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c66f5-e9ef-4816-a92b-6155a5ce7a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(hard-disk) Python",
   "language": "python",
   "name": "conda-env-hard-disk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
